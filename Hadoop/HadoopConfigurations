Hadoop Configurations:-

1> core-site.xml

In core-site.xml we are generally providing the configurations related to NameNodes for Hadoop Daemon.It contains the configuration settings for Hadoop Core such as I/O settings that are common to HDFS and MapReduce.
Ex:-
<configuration>
<property>
	<name>hadoop.tmp.dir</name>
	<value>/home/niraj/tmpdata</value>
</property>
<property>
	<name>fs.default.name</name>
	<value>hdfs://127.0.0.1:9000</value>
</property>
</configuration>

It can also Include properties like:-

a>fs.trash.interval => to specify the time period after which a trash checkpoint directory is deleted.(Specify in Minutes)
	or Number of minutes after which the checkpoint gets deleted. 
	Default Value - 0
b>proxy user configuration => Hadoop allows us to configure proxy users to submit jobs or access HDFS on behalf of other users; this is called impersonation. When you enable impersonation, any jobs 	   submitted using a proxy are executed with the impersonated user's existing privilege levels rather than those of a superuser (such as hdfs).
	To limit the hosts from which impersonated connections are allowed, use hadoop.proxyuser.<proxy_user>.hosts. For example, to allow user alice impersonated connections only from host_a and 		host_b:
		<property>
		   <name>hadoop.proxyuser.alice.hosts</name>
		   <value>host_a,host_b</value>
		</property>


c>io.compression.codecs => Linux supports GzipCodec, DefaultCodec, BZip2Codec, LzoCodec, and SnappyCodec for compression. Typically, GzipCodec is used for HDFS compression.

	To configure data compression, we can either enable a data compression codec.
	<property>
	  <name>io.compression.codecs</name>
	  <value>org.apache.hadoop.io.compress.GzipCodec,
	    org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.
	    LzoCodec,org.apache.hadoop.io.compress.SnappyCodec</value>
	  <description>A list of the compression codec classes that can be used
	    for compression/decompression.</description>
	</property>

d>net.topology.script.name => HDFS uses topology scripts to determine the rack location of nodes and uses this information to replicate block data to redundant racks.
	
	<property>
	  <name>net.topology.script.file.name</name> 
	  <value>/etc/hadoop/conf/rack-topology.sh</value>
	</property>
	
2> hdfs-site.xml

The hdfs-site.xml file contains the configuration settings for HDFS daemons; the NameNode, the Secondary NameNode, and the DataNodes.Here, we can configure hdfs-site.xml to specify default block replication and permission checking on HDFS. The actual number of replications can also be specified when the file is created. The default is used if replication is not specified in create time.
Ex:-
<configuration>
<property>
	<name>dfs.data.dir</name>
	<value>/home/niraj/dfsdata/namenode</value>
</property>
<property>
	<name>dfs.data.dir</name>
	<value>/home/niraj/dfsdata/datanode</value>
</property>
<property>
	<name>dfs.replication</name>
	<value>1</value>
</property>
</configuration>


It also includes properties like:-

a> dfs.blocksize => It is used to specify the size of block in bytes.Default Value is 128MB(134217728 Bytes)
	<property>
		<name>dfs.blocksize</name>
		<value>134217728</value>
	</property>

b> dfs.client.read.shortcircuit => It reads bypass the DataNode, allowing a client to read the file directly, as long as the client is co-located with the data. Short-circuit reads provide a 		  substantial performance boost to many applications and help improve HBase random read profile and Impala performance.
	<property>
	    <name>dfs.client.read.shortcircuit</name>
	    <value>true</value>
	</property>
c> dfs.namenode.http-address => On what ip address and port number name node services is running. 
	Default for Hadoop 3 => 0.0.0.0:9870
		    Hadoop 2 => 0.0.0.0:50070	
d> dfs.datanode.http-address => On what ip address and port number data node services is running.
	Default for Hadoop 3 => 0.0.0.0:9864
		    Hadoop 2 => 0.0.0.0:50075

	
 

	
	
